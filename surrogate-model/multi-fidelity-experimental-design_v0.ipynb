{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-fidelity Modeling and Experimental Design (Active Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "%matplotlib inline\n",
    "import GPy\n",
    "import emukit.multi_fidelity\n",
    "import emukit.test_functions\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyMultiOutputWrapper\n",
    "from emukit.multi_fidelity.models import GPyLinearMultiFidelityModel\n",
    "from emukit.multi_fidelity.convert_lists_to_array import convert_x_list_to_array, convert_xy_lists_to_arrays\n",
    "np.random.seed(20)\n",
    "\n",
    "from emukit.multi_fidelity.models.non_linear_multi_fidelity_model import make_non_linear_kernels, NonLinearMultiFidelityModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('Ge77Rates_v1.csv',\n",
    "                     dtype=None,\n",
    "                     delimiter=',',\n",
    "                    skip_header = 1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_l, x_train_h, y_train_l, y_train_h = ([],[],[],[])\n",
    "n_low_fidelity_points = 0\n",
    "n_high_fidelity_points = 0\n",
    "error_low = []\n",
    "error_hi = []\n",
    "for entry in data:\n",
    "    if entry[2] == b'LF':\n",
    "        x_train_l.append(entry[1])\n",
    "        y_train_l.append(entry[-4])\n",
    "        error_low.append(entry[-3])\n",
    "    else:\n",
    "        x_train_h.append(entry[1])\n",
    "        y_train_h.append(entry[-4])\n",
    "        error_hi.append(entry[-3])\n",
    "\n",
    "x_train_l, x_train_h, y_train_l, y_train_h = (np.atleast_2d(x_train_l).T, np.atleast_2d(x_train_h).T, np.atleast_2d(y_train_l).T, np.atleast_2d(y_train_h).T)\n",
    "\n",
    "X_train, Y_train = convert_xy_lists_to_arrays([x_train_l, x_train_h], [y_train_l, y_train_h])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x_train_l, y_train_l, color='b', s=40)\n",
    "plt.scatter(x_train_h, y_train_h, color='r', s=40)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f (x)')\n",
    "plt.xlim([0, 260])\n",
    "plt.legend(['Low fidelity', 'High fidelity'])\n",
    "plt.title('High and low fidelity functions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a linear multi-fidelity model\n",
    "\n",
    "kernels = [GPy.kern.RBF(1), GPy.kern.RBF(1)]\n",
    "lin_mf_kernel = emukit.multi_fidelity.kernels.LinearMultiFidelityKernel(kernels)\n",
    "gpy_lin_mf_model = GPyLinearMultiFidelityModel(X_train, Y_train, lin_mf_kernel, n_fidelities=2)\n",
    "gpy_lin_mf_model.mixed_noise.Gaussian_noise.fix(1e-6)\n",
    "'''\n",
    "The Low Fidelity noise level need to be independently estimated. Here I provide a guess of 5e-7\n",
    "'''\n",
    "gpy_lin_mf_model.mixed_noise.Gaussian_noise_1.fix(0)\n",
    "\n",
    "## Wrap the model using the given 'GPyMultiOutputWrapper'\n",
    "\n",
    "lin_mf_model = model = GPyMultiOutputWrapper(gpy_lin_mf_model, 2, n_optimization_restarts=20)\n",
    "\n",
    "## Fit the model\n",
    "  \n",
    "lin_mf_model.optimize()\n",
    "nonlin_mf_model = lin_mf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 200\n",
    "## Compute mean and variance predictions\n",
    "x_plot = np.linspace(0, 264, SPLIT)[:, None]\n",
    "X_plot = convert_x_list_to_array([x_plot, x_plot])\n",
    "\n",
    "hf_mean_nonlin_mf_model, hf_var_nonlin_mf_model = nonlin_mf_model.predict(X_plot[:SPLIT])\n",
    "hf_std_nonlin_mf_model = np.sqrt(hf_var_nonlin_mf_model)\n",
    "\n",
    "lf_mean_nonlin_mf_model, lf_var_nonlin_mf_model = nonlin_mf_model.predict(X_plot[SPLIT:])\n",
    "lf_std_nonlin_mf_model = np.sqrt(lf_var_nonlin_mf_model)\n",
    "\n",
    "\n",
    "## Plot posterior mean and variance of nonlinear multi-fidelity model\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.fill_between(x_plot.flatten(), (lf_mean_nonlin_mf_model - 1.96*lf_std_nonlin_mf_model).flatten(), \n",
    "                 (lf_mean_nonlin_mf_model + 1.96*lf_std_nonlin_mf_model).flatten(), color='g', alpha=0.3)\n",
    "plt.fill_between(x_plot.flatten(), (hf_mean_nonlin_mf_model - 1.96*hf_std_nonlin_mf_model).flatten(), \n",
    "                 (hf_mean_nonlin_mf_model + 1.96*hf_std_nonlin_mf_model).flatten(), color='y', alpha=0.3)\n",
    "plt.plot(x_train_l, y_train_l, 'b',marker=\".\")\n",
    "plt.plot(x_train_h, y_train_h, 'r',marker=\".\")\n",
    "plt.plot(x_plot, lf_mean_nonlin_mf_model, '--', color='g')\n",
    "plt.plot(x_plot, hf_mean_nonlin_mf_model, '--', color='y')\n",
    "plt.scatter(x_train_h, y_train_h, color='r')\n",
    "plt.xlabel('Radii')\n",
    "plt.ylabel('Ge77 Production Rate')\n",
    "plt.xlim(0, 250)\n",
    "plt.legend(['Low Fidelity', 'High Fidelity', 'Predicted Low Fidelity', 'Predicted High Fidelity'])\n",
    "plt.title('linear multi-fidelity model fit to low and high fidelity functions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acqusition Curve\n",
    "- The acquisition curve is an important part of the active learning process. The next step we try using HF simulation dependes on where the acquisiton function takes its maximal value.\n",
    "- Define a parameter space (here we only have a single parameter radius), we need to add another parameter fidelity into our data, this parameter is always 1, meaning that we always run acquisition function on the high fidelity (1) space.\n",
    "- Note: it is important to deine the lower and upper range of our optimization parameter. Looking at the previous plot, anything below 80cm is probably unphysical, therefore we should not waste any attempt there. I selected a region of 90-250 to run the acquisition function. Selecting the wrong range could significantly change the shape of acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from emukit.experimental_design.acquisitions import ModelVariance,IntegratedVarianceReduction\n",
    "from emukit.core import ParameterSpace, ContinuousParameter,DiscreteParameter\n",
    "SPLIT = 200\n",
    "low = 90\n",
    "high =250\n",
    "## Compute mean and variance predictions\n",
    "x_plot = np.linspace(low, high, SPLIT)[:, None]\n",
    "X_plot = convert_x_list_to_array([x_plot, x_plot])\n",
    "parameter_space = ParameterSpace([ContinuousParameter('x1', low, high), DiscreteParameter(\"f\",[1])])\n",
    "us_acquisition = IntegratedVarianceReduction(nonlin_mf_model, parameter_space)\n",
    "acq = us_acquisition.evaluate(X_plot[SPLIT:])\n",
    "plt.plot(np.linspace(low, high, SPLIT),acq/acq.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run a gradient-based optimizer over the acquisition function to find the next point to attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from emukit.core.optimization import GradientAcquisitionOptimizer\n",
    "optimizer = GradientAcquisitionOptimizer(parameter_space)\n",
    "x_new, _ = optimizer.optimize(us_acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x_plot, acq / np.max(acq), \"green\", label=\"US\")\n",
    "plt.axvline(x_new[0,0], color=\"red\", label=\"x_next\", linestyle=\"--\")\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we attempted another HF simulation at `x = 240.01400146`, and obtain the Ge77 production rate to be `0.012`. Then we can prepare this new HF dataset and add it back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_new_data = np.array([[240.01400146,1.0]])\n",
    "y_new_data = np.array([[0.012]])\n",
    "X_train = np.append(X_train,x_new_data,axis=0)\n",
    "Y_train = np.append(Y_train,y_new_data,axis=0)\n",
    "nonlin_mf_model.set_data(X_train, Y_train)\n",
    "\n",
    "SPLIT = 200\n",
    "## Compute mean and variance predictions\n",
    "x_plot = np.linspace(0, 264, SPLIT)[:, None]\n",
    "X_plot = convert_x_list_to_array([x_plot, x_plot])\n",
    "\n",
    "hf_mean_nonlin_mf_model, hf_var_nonlin_mf_model = nonlin_mf_model.predict(X_plot[:SPLIT])\n",
    "hf_std_nonlin_mf_model = np.sqrt(hf_var_nonlin_mf_model)\n",
    "\n",
    "lf_mean_nonlin_mf_model, lf_var_nonlin_mf_model = nonlin_mf_model.predict(X_plot[SPLIT:])\n",
    "lf_std_nonlin_mf_model = np.sqrt(lf_var_nonlin_mf_model)\n",
    "\n",
    "\n",
    "## Plot posterior mean and variance of nonlinear multi-fidelity model\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.fill_between(x_plot.flatten(), (lf_mean_nonlin_mf_model - 1.96*lf_std_nonlin_mf_model).flatten(), \n",
    "                 (lf_mean_nonlin_mf_model + 1.96*lf_std_nonlin_mf_model).flatten(), color='g', alpha=0.3)\n",
    "plt.fill_between(x_plot.flatten(), (hf_mean_nonlin_mf_model - 1.96*hf_std_nonlin_mf_model).flatten(), \n",
    "                 (hf_mean_nonlin_mf_model + 1.96*hf_std_nonlin_mf_model).flatten(), color='y', alpha=0.3)\n",
    "plt.plot(x_train_l, y_train_l, 'b',marker=\".\")\n",
    "plt.plot(x_train_h, y_train_h, 'r',marker=\".\")\n",
    "plt.scatter([x_new_data[0,0]], [y_new_data[0,0]],color=\"orange\")\n",
    "plt.plot(x_plot, lf_mean_nonlin_mf_model, '--', color='g')\n",
    "plt.plot(x_plot, hf_mean_nonlin_mf_model, '--', color='y')\n",
    "plt.scatter(x_train_h, y_train_h, color='r')\n",
    "plt.xlabel('Radii')\n",
    "plt.ylabel('Ge77 Production Rate')\n",
    "plt.xlim(0, 250)\n",
    "plt.legend(['Low Fidelity', 'High Fidelity','Predicted Low Fidelity', 'Predicted High Fidelity',\"Newly Attempted HF Point\"])\n",
    "plt.title('linear multi-fidelity model fit to low and high fidelity functions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rerun the acquisition function and gradient finding function, we will see that it suggests a next point to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "us_acquisition = IntegratedVarianceReduction(nonlin_mf_model, parameter_space)\n",
    "acq = us_acquisition.evaluate(X_plot[SPLIT:])\n",
    "optimizer = GradientAcquisitionOptimizer(parameter_space)\n",
    "x_new, _ = optimizer.optimize(us_acquisition)\n",
    "print(x_new)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x_plot, acq / np.max(acq), \"green\", label=\"IVR\")\n",
    "plt.axvline(x_new[0,0], color=\"red\", label=\"x_next\", linestyle=\"--\")\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$f(x)$\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it suggests us to attempt 161cm as the next step. We can keep looping through these procedure until we are confident with our final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
